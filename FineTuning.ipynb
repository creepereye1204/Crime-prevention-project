{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmnPhlgEc+H5wBOygmu73X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c93c35c995c4922838fc18cb07a5ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_099587703c5745eeaf40c80b61750519",
              "IPY_MODEL_89bf72c1dd51421e8dd8620c96809e10",
              "IPY_MODEL_dfd72a43c98143b0a4944a3e2e70e92a"
            ],
            "layout": "IPY_MODEL_b721898cf9354428ba2b579612ddf9b6"
          }
        },
        "099587703c5745eeaf40c80b61750519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d4af78e0254d73baa90183a06d8de7",
            "placeholder": "​",
            "style": "IPY_MODEL_e932dfe091cf4012b1a84e87210de4bb",
            "value": "100%"
          }
        },
        "89bf72c1dd51421e8dd8620c96809e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb067f887e654d619f91712306cec38a",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a8eda12291946398bd2a3d6a1f427bd",
            "value": 111898327
          }
        },
        "dfd72a43c98143b0a4944a3e2e70e92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4668c409c89048aea43fdf293a702091",
            "placeholder": "​",
            "style": "IPY_MODEL_85637db8650041cbae1e88fb5ddcd30f",
            "value": " 107M/107M [00:00&lt;00:00, 201MB/s]"
          }
        },
        "b721898cf9354428ba2b579612ddf9b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d4af78e0254d73baa90183a06d8de7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e932dfe091cf4012b1a84e87210de4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb067f887e654d619f91712306cec38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8eda12291946398bd2a3d6a1f427bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4668c409c89048aea43fdf293a702091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85637db8650041cbae1e88fb5ddcd30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/creepereye1204/Crime-prevention-project/blob/renewal/FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset/\n",
        "#     anchor/\n",
        "#         anchor1.jpg\n",
        "#         anchor2.jpg\n",
        "#         ...\n",
        "#     positive/\n",
        "#         positive1.jpg\n",
        "#         positive2.jpg\n",
        "#         ...\n",
        "#     negative/\n",
        "#         negative1.jpg\n",
        "#         negative2.jpg\n",
        "#         ...\n"
      ],
      "metadata": {
        "id": "JuW-eOGu2tPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CPU로"
      ],
      "metadata": {
        "id": "5_TFQX1B3GYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctAyHHDL1Yx8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train()\n",
        "\n",
        "# 2. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 3. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU로"
      ],
      "metadata": {
        "id": "ZWtGB9-z2AI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.utils.serialization as xser\n",
        "\n",
        "# 1. TPU 장치 설정\n",
        "device = xm.xla_device()\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 4. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "FKghI9dw3LzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU로"
      ],
      "metadata": {
        "id": "rwOhPEgw3NQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 4. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "UVZ3jEUn3O8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링"
      ],
      "metadata": {
        "id": "2mUvurFr5Lv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 검색어 설정\n",
        "query = '한국 연예인'  # 검색어를 변경할 수 있습니다.\n",
        "url = f'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={query}'\n",
        "\n",
        "# 요청 헤더 설정 (네이버는 User-Agent 확인을 통해 봇을 차단할 수 있음)\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 이미지 URL 수집\n",
        "image_urls = []\n",
        "for img_tag in soup.find_all('img'):\n",
        "    try:\n",
        "        img_url = img_tag['data-source']\n",
        "        image_urls.append(img_url)\n",
        "    except KeyError:\n",
        "        # data-source 속성이 없는 경우 건너뜀\n",
        "        continue\n",
        "\n",
        "# 이미지 다운로드 디렉터리 설정\n",
        "os.makedirs('korean_celeb_images', exist_ok=True)\n",
        "\n",
        "# 이미지 다운로드\n",
        "for i, img_url in enumerate(image_urls):\n",
        "    try:\n",
        "        img_response = requests.get(img_url)\n",
        "        img_response.raise_for_status()  # 요청이 성공했는지 확인\n",
        "        with open(f'korean_celeb_images/korean_celeb_{i}.jpg', 'wb') as file:\n",
        "            file.write(img_response.content)\n",
        "        print(f'Downloaded image {i + 1}')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f'Failed to download image {i + 1}: {e}')\n",
        "\n",
        "print('이미지 다운로드 완료')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDSQ2uPg5NP8",
        "outputId": "66fdaf8d-eef1-4428-b9ba-52c424cfd46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이미지 다운로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPsf3xqLmNQ",
        "outputId": "4a9e03d2-44a7-44dd-dab2-62d058bcc8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.31.0)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.2.2)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aOGJ8EoE6CN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "outputId": "7744b9fc-0b04-4701-f932-a464c314683d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이미지 찾을 대상을 입력해주세요.백종원\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-567b99b96305>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 크롬 드라이버 설치\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 구글 사이트 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/chrome.py\u001b[0m in \u001b[0;36minstall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdriver_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_driver_binary_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o755\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdriver_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/core/manager.py\u001b[0m in \u001b[0;36m_get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mos_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_os_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_driver_download_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mbinary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_file_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/drivers/chrome.py\u001b[0m in \u001b[0;36mget_driver_download_url\u001b[0;34m(self, os_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_driver_download_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdriver_version_to_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_driver_version_to_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# For Mac ARM CPUs after version 106.0.5249.61 the format of OS type changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# to more unified \"mac_arm64\". For newer versions, it'll be \"mac_arm64\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/core/driver.py\u001b[0m in \u001b[0;36mget_driver_version_to_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver_version_to_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latest_release_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_latest_release_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/drivers/chrome.py\u001b[0m in \u001b[0;36mget_latest_release_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdetermined_browser_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Remove the build version (the last segment) from determined_browser_version for version < 113\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdetermined_browser_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetermined_browser_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         latest_release_url = (\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_release_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = sorted(os.listdir(anchor_dir))\n",
        "        self.positive_images = sorted(os.listdir(positive_dir))\n",
        "        self.negative_images = sorted(os.listdir(negative_dir))\n",
        "\n",
        "        # 앵커 이미지에 해당하는 양성 이미지를 매핑합니다.\n",
        "        self.anchor_to_positives = self._map_anchor_to_positives()\n",
        "\n",
        "    def _map_anchor_to_positives(self):\n",
        "        anchor_to_positives = {}\n",
        "        for positive_image in self.positive_images:\n",
        "            anchor_name = positive_image.split('_')[0]\n",
        "            if anchor_name not in anchor_to_positives:\n",
        "                anchor_to_positives[anchor_name] = []\n",
        "            anchor_to_positives[anchor_name].append(positive_image)\n",
        "        return anchor_to_positives\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_image_name = self.anchor_images[idx]\n",
        "        anchor_path = os.path.join(self.anchor_dir, anchor_image_name)\n",
        "\n",
        "        # 앵커 이미지에 대응하는 양성 이미지 중 하나를 랜덤하게 선택합니다.\n",
        "        positive_image_name = random.choice(self.anchor_to_positives[anchor_image_name.split('.')[0]])\n",
        "        positive_path = os.path.join(self.positive_dir, positive_image_name)\n",
        "\n",
        "        # 음성 이미지는 무작위로 선택합니다.\n",
        "        negative_image_name = random.choice(self.negative_images)\n",
        "        negative_path = os.path.join(self.negative_dir, negative_image_name)\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "v-GfANlELhHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "z6JImhd--mdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가(evaluation) 함수 정의\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():  # 기울기 계산을 비활성화하여 메모리 사용량 줄이고 계산 속도 향상\n",
        "        for data in dataloader:\n",
        "            anchor, positive, negative = data\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            anchor_embedding = model(anchor)\n",
        "            positive_embedding = model(positive)\n",
        "            negative_embedding = model(negative)\n",
        "            loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    print(f'Evaluation Loss: {avg_loss:.3f}')\n",
        "    return avg_loss\n",
        "\n",
        "# 테스트 데이터셋과 데이터로더 준비\n",
        "# 테스트 데이터셋의 구조는 학습 및 평가 데이터셋과 동일하다고 가정\n",
        "test_dataset = TripletDataset('test_dataset/', transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "WVSY61Kb2q9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "dhaMDNaAAIQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset/\n",
        "│\n",
        "├── Person1/\n",
        "│   ├── anchor_001.jpg  # 앵커 이미지\n",
        "│   ├── positive_001.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── positive_002.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── negative_001.jpg  # 부정 이미지 (다른 클래스)\n",
        "│   └── negative_002.jpg  # 부정 이미지 (다른 클래스)\n",
        "│\n",
        "├── Person2/\n",
        "│   ├── anchor_001.jpg  # 앵커 이미지\n",
        "│   ├── positive_001.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── positive_002.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── negative_001.jpg  # 부정 이미지 (다른 클래스)\n",
        "│   └── negative_002.jpg  # 부정 이미지 (다른 클래스)\n",
        "│\n",
        "├── Person3/\n",
        "│   ├── anchor_001.jpg  # 앵커 이미지\n",
        "│   ├── positive_001.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── positive_002.jpg  # 긍정 이미지 (같은 클래스)\n",
        "│   ├── negative_001.jpg  # 부정 이미지 (다른 클래스)\n",
        "│   └── negative_002.jpg  # 부정 이미지 (다른 클래스)\n",
        "│\n",
        "└── ...\n"
      ],
      "metadata": {
        "id": "-hH4b94SyWjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네, 제공하신 구조에 따라 이미지를 저장하는 방법은 트리플렛 네트워크 학습에 적합합니다. 각 '앵커' 폴더에는 해당 앵커 이미지와 그에 대응하는 긍정(positive) 및 부정(negative) 이미지가 포함되어 있습니다. 이 구조는 앵커 이미지와 긍정 이미지 간의 유사성을 학습하고, 앵커 이미지와 부정 이미지 간의 차이를 학습하는 데 도움이 됩니다.\n",
        "\n",
        "# 그러나, 실제 코드에서 이미지 경로를 만들 때 'anchor', 'positive', 'negative'라는 단어를 포함하는 방식에 따라 이미지를 분류하므로, 폴더 구조와 이미지의 이름을 명확하게 정의해야 합니다. 제공하신 예시에서는 각 앵커 폴더에 하나의 'anchor.jpg' 이미지, 여러 개의 'positive_xx.jpg', 그리고 여러 개의 'negative_xx.jpg' 이미지가 포함되어 있습니다. 이 구조는 코드에서 정의한 `TripletDataset` 클래스의 `make_dataset` 메소드 로직과 일치합니다.\n",
        "\n",
        "# `TripletDataset` 클래스의 `make_dataset` 메소드는 각 앵커 이미지에 대해 가능한 모든 긍정 및 부정 이미지 조합을 찾아 이미지의 트리플렛(앵커, 긍정, 부정)을 생성합니다. 이렇게 하여 학습 데이터셋에 다양성을 부여하고, 모델이 더 강력한 특징을 학습할 수 있도록 합니다.\n",
        "\n",
        "# 따라서, 제공하신 파일 구조는 코드의 요구사항을 충족하며, 트리플렛 네트워크 학습에 적합한 방식입니다. 하지만, 실제 프로젝트에서는 클래스별로 더 많은 이미지를 포함시키고, 다양한 앵커, 긍정, 부정 이미지 조합을 실험해 볼 필요가 있습니다. 이를 통해 모델의 일반화 능력을 향상시키고, 실제 세계의 다양한 시나리오에 대응할 수 있는 robust한 모델을 개발할 수 있습니다.\n",
        "\n",
        "# 이런 자료를 참고했어요.\n",
        "# [1] AI-Hub - 한국어 이미지 설명 데이터셋 (https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=261)\n",
        "# [2] 티스토리 - [PyTorch] 4-1. 나만의 이미지 데이터셋 만들기 - Real Late Starter (https://data-panic.tistory.com/13)\n",
        "# [3] Superb AI - 컴퓨터 비전 데이터셋 - 공공 데이터셋 살펴보기 - 슈퍼브 블로그 (https://blog-ko.superb-ai.com/exploring-computer-vision-datasets/)\n",
        "# [4] GitHub - 작은 데이터셋으로 강력한 이미지 분류 모델 설계하기 (https://keraskorea.github.io/posts/2018-10-24-little_data_powerful_model/)\n",
        "\n",
        "# 뤼튼 사용하러 가기 > https://agent.wrtn.ai/5xb91l"
      ],
      "metadata": {
        "id": "UsvQ3njvAOmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "-YQBye-e0Chy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L5h7Y4QGxZfN",
        "outputId": "ea7daf67-d9b8-42a7-dcca-00c7e1504afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet_pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (1.25.2)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet_pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (2.31.0)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet_pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m775.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.18.0,>=0.17.0 (from facenet_pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet_pytorch) (1.3.0)\n",
            "Installing collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet_pytorch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet_pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "45bea49d952f41bc88d18e0e7a2007b6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## 이코드가 진짜"
      ],
      "metadata": {
        "id": "FV1ARFflwQhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path).convert('RGB')\n",
        "        positive_img = Image.open(positive_path).convert('RGB')\n",
        "        negative_img = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "# 데이터셋 나누기\n",
        "def split_dataset(dataset, train_ratio=0.7, eval_ratio=0.1, test_ratio=0.2):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    eval_size = int(eval_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - eval_size\n",
        "    return random_split(dataset, [train_size, eval_size, test_size])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "train_dataset, eval_dataset, test_dataset = split_dataset(dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # 검증 루프\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(eval_loader, 0):\n",
        "            anchor, positive, negative = data\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            anchor_embedding = model(anchor)\n",
        "            positive_embedding = model(positive)\n",
        "            negative_embedding = model(negative)\n",
        "            loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    print(f'[Epoch {epoch + 1}] Eval loss: {eval_loss / len(eval_loader):.3f}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 테스트 루프\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f'Test loss: {test_loss / len(test_loader):.3f}')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# # 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "# anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "# positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "# negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "# anchor_embedding = get_embedding(model, anchor_image)\n",
        "# positive_embedding = get_embedding(model, positive_image)\n",
        "# negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# # 유클리드 거리 계산\n",
        "# pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "\n",
        "# # 거리 출력\n",
        "# print(f\"양성 이미지와의 유클리드 거리: {pos_distance():.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a9Etfsfi4nN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479,
          "referenced_widgets": [
            "0c93c35c995c4922838fc18cb07a5ef5",
            "099587703c5745eeaf40c80b61750519",
            "89bf72c1dd51421e8dd8620c96809e10",
            "dfd72a43c98143b0a4944a3e2e70e92a",
            "b721898cf9354428ba2b579612ddf9b6",
            "d8d4af78e0254d73baa90183a06d8de7",
            "e932dfe091cf4012b1a84e87210de4bb",
            "cb067f887e654d619f91712306cec38a",
            "6a8eda12291946398bd2a3d6a1f427bd",
            "4668c409c89048aea43fdf293a702091",
            "85637db8650041cbae1e88fb5ddcd30f"
          ]
        },
        "outputId": "baea5e90-8c6b-4061-e229-184da0127946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c93c35c995c4922838fc18cb07a5ef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:449: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:449: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d650cdc43f51>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0meval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 진짜코드위에"
      ],
      "metadata": {
        "id": "-CRkZ5UNwv1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def expand_batch(input_tensor, target_batch_size):\n",
        "    current_batch_size = input_tensor.size(0)\n",
        "    if current_batch_size == 1:\n",
        "        # input_tensor을 target_batch_size만큼 복사하여 새로운 배치를 만듭니다.\n",
        "        expanded_tensor = input_tensor.repeat(target_batch_size, 1, 1, 1)\n",
        "        return expanded_tensor\n",
        "    else:\n",
        "        return input_tensor\n",
        "\n",
        "# 예제 입력 텐서\n",
        "anchor = torch.randn(1, 3, 224, 224)  # 현재 배치 크기가 1인 경우\n",
        "\n"
      ],
      "metadata": {
        "id": "IpZBdiUhwI64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dvc데이터 버전컨트롤,\n",
        "#(git, github, svn)"
      ],
      "metadata": {
        "id": "dUYx16DIHTgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 버전 관리중요"
      ],
      "metadata": {
        "id": "HQKPjuv9HoTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃헙:CI,CD,협업 깃헙은 깃을 클라우드에서 쓰게만들어주는것"
      ],
      "metadata": {
        "id": "VfIAeKXiII62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 관리: mlflow,web tool, bentoml"
      ],
      "metadata": {
        "id": "NAhOOpQSHt8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가정->임의성->데이터->설정->결과"
      ],
      "metadata": {
        "id": "eAD9M0aMI-cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hydra,오메가콘프"
      ],
      "metadata": {
        "id": "io6YWPaiMhRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mamba,conda,poetry"
      ],
      "metadata": {
        "id": "gHrQim8-Mya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 재현가능성,"
      ],
      "metadata": {
        "id": "9Pn04npPM9la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 컨테이너 도커"
      ],
      "metadata": {
        "id": "BkE_lRZNNfQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# git , dvc , docker"
      ],
      "metadata": {
        "id": "tpPvKa2zQ90a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃,깃헙,도커 는 필수\n",
        "#자동환경(깃헙 액션)"
      ],
      "metadata": {
        "id": "LlpPndv-ODVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch complie"
      ],
      "metadata": {
        "id": "dkeQs64rSnYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ONNX 로 포멧변경 을하여 임베딩즉 디바이스에 적용"
      ],
      "metadata": {
        "id": "BoMrVdbSSMAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 양자화, 가지치기, 증류"
      ],
      "metadata": {
        "id": "ZJPewYlETXG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, drop_last=True)  # drop_last=True 추가\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        target_batch_size = 32  # 원하는 배치 크기\n",
        "\n",
        "        # 가짜 배치 크기로 확장\n",
        "        anchor = expand_batch(anchor, target_batch_size)\n",
        "        positive = expand_batch(positive, target_batch_size)\n",
        "        negative = expand_batch(negative, target_batch_size)\n",
        "\n",
        "        # 모델에 입력\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "        print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "        running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NMDZNdMmtWuW",
        "outputId": "f0f78952-0a44-4fad-efc5-912d1e630d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 1] loss: 0.103\n",
            "[Epoch 2, Batch 1] loss: 0.099\n",
            "[Epoch 3, Batch 1] loss: 0.100\n",
            "[Epoch 4, Batch 1] loss: 0.099\n",
            "[Epoch 5, Batch 1] loss: 0.103\n",
            "[Epoch 6, Batch 1] loss: 0.099\n",
            "[Epoch 7, Batch 1] loss: 0.106\n",
            "[Epoch 8, Batch 1] loss: 0.098\n",
            "[Epoch 9, Batch 1] loss: 0.102\n",
            "[Epoch 10, Batch 1] loss: 0.095\n",
            "[Epoch 11, Batch 1] loss: 0.099\n",
            "[Epoch 12, Batch 1] loss: 0.100\n",
            "[Epoch 13, Batch 1] loss: 0.100\n",
            "[Epoch 14, Batch 1] loss: 0.098\n",
            "[Epoch 15, Batch 1] loss: 0.098\n",
            "[Epoch 16, Batch 1] loss: 0.098\n",
            "[Epoch 17, Batch 1] loss: 0.099\n",
            "[Epoch 18, Batch 1] loss: 0.096\n",
            "[Epoch 19, Batch 1] loss: 0.101\n",
            "[Epoch 20, Batch 1] loss: 0.102\n",
            "[Epoch 21, Batch 1] loss: 0.102\n",
            "[Epoch 22, Batch 1] loss: 0.095\n",
            "[Epoch 23, Batch 1] loss: 0.094\n",
            "[Epoch 24, Batch 1] loss: 0.104\n",
            "[Epoch 25, Batch 1] loss: 0.097\n",
            "[Epoch 26, Batch 1] loss: 0.106\n",
            "[Epoch 27, Batch 1] loss: 0.098\n",
            "[Epoch 28, Batch 1] loss: 0.108\n",
            "[Epoch 29, Batch 1] loss: 0.094\n",
            "[Epoch 30, Batch 1] loss: 0.097\n",
            "[Epoch 31, Batch 1] loss: 0.102\n",
            "[Epoch 32, Batch 1] loss: 0.097\n",
            "[Epoch 33, Batch 1] loss: 0.100\n",
            "[Epoch 34, Batch 1] loss: 0.102\n",
            "[Epoch 35, Batch 1] loss: 0.101\n",
            "[Epoch 36, Batch 1] loss: 0.102\n",
            "[Epoch 37, Batch 1] loss: 0.098\n",
            "[Epoch 38, Batch 1] loss: 0.102\n",
            "[Epoch 39, Batch 1] loss: 0.099\n",
            "[Epoch 40, Batch 1] loss: 0.101\n",
            "[Epoch 41, Batch 1] loss: 0.100\n",
            "[Epoch 42, Batch 1] loss: 0.102\n",
            "[Epoch 43, Batch 1] loss: 0.101\n",
            "[Epoch 44, Batch 1] loss: 0.096\n",
            "[Epoch 45, Batch 1] loss: 0.102\n",
            "[Epoch 46, Batch 1] loss: 0.101\n",
            "[Epoch 47, Batch 1] loss: 0.100\n",
            "[Epoch 48, Batch 1] loss: 0.102\n",
            "[Epoch 49, Batch 1] loss: 0.103\n",
            "[Epoch 50, Batch 1] loss: 0.099\n",
            "[Epoch 51, Batch 1] loss: 0.099\n",
            "[Epoch 52, Batch 1] loss: 0.102\n",
            "[Epoch 53, Batch 1] loss: 0.100\n",
            "[Epoch 54, Batch 1] loss: 0.104\n",
            "[Epoch 55, Batch 1] loss: 0.104\n",
            "[Epoch 56, Batch 1] loss: 0.104\n",
            "[Epoch 57, Batch 1] loss: 0.105\n",
            "[Epoch 58, Batch 1] loss: 0.103\n",
            "[Epoch 59, Batch 1] loss: 0.094\n",
            "[Epoch 60, Batch 1] loss: 0.101\n",
            "[Epoch 61, Batch 1] loss: 0.099\n",
            "[Epoch 62, Batch 1] loss: 0.102\n",
            "[Epoch 63, Batch 1] loss: 0.102\n",
            "[Epoch 64, Batch 1] loss: 0.103\n",
            "[Epoch 65, Batch 1] loss: 0.100\n",
            "[Epoch 66, Batch 1] loss: 0.103\n",
            "[Epoch 67, Batch 1] loss: 0.096\n",
            "[Epoch 68, Batch 1] loss: 0.098\n",
            "[Epoch 69, Batch 1] loss: 0.097\n",
            "[Epoch 70, Batch 1] loss: 0.096\n",
            "[Epoch 71, Batch 1] loss: 0.102\n",
            "[Epoch 72, Batch 1] loss: 0.099\n",
            "[Epoch 73, Batch 1] loss: 0.102\n",
            "[Epoch 74, Batch 1] loss: 0.100\n",
            "[Epoch 75, Batch 1] loss: 0.099\n",
            "[Epoch 76, Batch 1] loss: 0.099\n",
            "[Epoch 77, Batch 1] loss: 0.099\n",
            "[Epoch 78, Batch 1] loss: 0.101\n",
            "[Epoch 79, Batch 1] loss: 0.098\n",
            "[Epoch 80, Batch 1] loss: 0.097\n",
            "[Epoch 81, Batch 1] loss: 0.102\n",
            "[Epoch 82, Batch 1] loss: 0.100\n",
            "[Epoch 83, Batch 1] loss: 0.101\n",
            "[Epoch 84, Batch 1] loss: 0.103\n",
            "[Epoch 85, Batch 1] loss: 0.099\n",
            "[Epoch 86, Batch 1] loss: 0.098\n",
            "[Epoch 87, Batch 1] loss: 0.104\n",
            "[Epoch 88, Batch 1] loss: 0.101\n",
            "[Epoch 89, Batch 1] loss: 0.098\n",
            "[Epoch 90, Batch 1] loss: 0.099\n",
            "[Epoch 91, Batch 1] loss: 0.100\n",
            "[Epoch 92, Batch 1] loss: 0.100\n",
            "[Epoch 93, Batch 1] loss: 0.104\n",
            "[Epoch 94, Batch 1] loss: 0.103\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0793e7a9b7ef>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EKfEW-Hayrqo",
        "outputId": "640a59a4-7104-4788-c845-d81457b7d235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet_pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (1.25.2)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet_pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (2.31.0)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet_pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.18.0,>=0.17.0 (from facenet_pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet_pytorch) (1.3.0)\n",
            "Installing collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet_pytorch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet_pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "d7e634aee74545e7b69f6526da43b350"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eC8ZXTdAAyxI",
        "outputId": "f7c9bc5f-ecb1-4433-aa2f-1bd89cceb47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 1] loss: 0.102\n",
            "[Epoch 2, Batch 1] loss: 0.100\n",
            "[Epoch 3, Batch 1] loss: 0.103\n",
            "[Epoch 4, Batch 1] loss: 0.099\n",
            "[Epoch 5, Batch 1] loss: 0.097\n",
            "[Epoch 6, Batch 1] loss: 0.100\n",
            "[Epoch 7, Batch 1] loss: 0.099\n",
            "[Epoch 8, Batch 1] loss: 0.099\n",
            "[Epoch 9, Batch 1] loss: 0.103\n",
            "[Epoch 10, Batch 1] loss: 0.102\n",
            "[Epoch 11, Batch 1] loss: 0.101\n",
            "[Epoch 12, Batch 1] loss: 0.099\n",
            "[Epoch 13, Batch 1] loss: 0.099\n",
            "[Epoch 14, Batch 1] loss: 0.101\n",
            "[Epoch 15, Batch 1] loss: 0.096\n",
            "[Epoch 16, Batch 1] loss: 0.098\n",
            "[Epoch 17, Batch 1] loss: 0.100\n",
            "[Epoch 18, Batch 1] loss: 0.099\n",
            "[Epoch 19, Batch 1] loss: 0.096\n",
            "[Epoch 20, Batch 1] loss: 0.098\n",
            "[Epoch 21, Batch 1] loss: 0.097\n",
            "[Epoch 22, Batch 1] loss: 0.098\n",
            "[Epoch 23, Batch 1] loss: 0.104\n",
            "[Epoch 24, Batch 1] loss: 0.098\n",
            "[Epoch 25, Batch 1] loss: 0.098\n",
            "[Epoch 26, Batch 1] loss: 0.100\n",
            "[Epoch 27, Batch 1] loss: 0.094\n",
            "[Epoch 28, Batch 1] loss: 0.097\n",
            "[Epoch 29, Batch 1] loss: 0.101\n",
            "[Epoch 30, Batch 1] loss: 0.098\n",
            "[Epoch 31, Batch 1] loss: 0.101\n",
            "[Epoch 32, Batch 1] loss: 0.102\n",
            "[Epoch 33, Batch 1] loss: 0.097\n",
            "[Epoch 34, Batch 1] loss: 0.099\n",
            "[Epoch 35, Batch 1] loss: 0.104\n",
            "[Epoch 36, Batch 1] loss: 0.099\n",
            "[Epoch 37, Batch 1] loss: 0.100\n",
            "[Epoch 38, Batch 1] loss: 0.100\n",
            "[Epoch 39, Batch 1] loss: 0.097\n",
            "[Epoch 40, Batch 1] loss: 0.100\n",
            "[Epoch 41, Batch 1] loss: 0.098\n",
            "[Epoch 42, Batch 1] loss: 0.099\n",
            "[Epoch 43, Batch 1] loss: 0.099\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-16647dd7a43d>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnegative_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            for i in range(len(img_names) - 1):\n",
        "                anchor_img = img_names[i]\n",
        "                positive_img = img_names[i + 1]\n",
        "                for other_class_name in self.classes:\n",
        "                    if other_class_name != class_name:\n",
        "                        other_class_path = os.path.join(self.dataset_path, other_class_name)\n",
        "                        negative_img = random.choice(os.listdir(other_class_path))\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(other_class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        target_batch_size = 32  # 원하는 배치 크기\n",
        "\n",
        "        # 가짜 배치 크기로 확장\n",
        "        anchor = expand_batch(anchor, target_batch_size)\n",
        "        positive = expand_batch(positive, target_batch_size)\n",
        "        negative = expand_batch(negative, target_batch_size)\n",
        "\n",
        "        # 모델에 입력\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "        running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "m-2dtHUuA05F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################"
      ],
      "metadata": {
        "id": "ogWGY6vsU9QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "# GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            for img_name in img_names:\n",
        "                anchor_img = img_name\n",
        "                positive_img = random.choice([img for img in img_names if img != anchor_img])\n",
        "                negative_class = random.choice([cls for cls in self.classes if cls != class_name])\n",
        "                negative_class_path = os.path.join(self.dataset_path, negative_class)\n",
        "                negative_img = random.choice(os.listdir(negative_class_path))\n",
        "                imgs.append((os.path.join(class_path, anchor_img),\n",
        "                             os.path.join(class_path, positive_img),\n",
        "                             os.path.join(negative_class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path).convert('RGB')\n",
        "        positive_img = Image.open(positive_path).convert('RGB')\n",
        "        negative_img = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "# 데이터셋 나누기\n",
        "def split_dataset(dataset, train_ratio=0.7, eval_ratio=0.1, test_ratio=0.2):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    eval_size = int(eval_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - eval_size\n",
        "    return random_split(dataset, [train_size, eval_size, test_size])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "train_dataset, eval_dataset, test_dataset = split_dataset(dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # 검증 루프\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(eval_loader, 0):\n",
        "            anchor, positive, negative = data\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            anchor_embedding = model(anchor)\n",
        "            positive_embedding = model(positive)\n",
        "            negative_embedding = model(negative)\n",
        "            loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    print(f'[Epoch {epoch + 1}] Eval loss: {eval_loss / len(eval_loader):.3f}')\n",
        "print('Finished Training')\n",
        "\n",
        "# 테스트 루프\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        test_loss += loss.item()\n",
        "print(f'Test loss: {test_loss / len(test_loader):.3f}')\n",
        "\n",
        "# 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "FAb1y9VOU92F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}