{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrWrdPM4RFMxin4Xqi4j54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/creepereye1204/Crime-prevention-project/blob/renewal/FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset/\n",
        "#     anchor/\n",
        "#         anchor1.jpg\n",
        "#         anchor2.jpg\n",
        "#         ...\n",
        "#     positive/\n",
        "#         positive1.jpg\n",
        "#         positive2.jpg\n",
        "#         ...\n",
        "#     negative/\n",
        "#         negative1.jpg\n",
        "#         negative2.jpg\n",
        "#         ...\n"
      ],
      "metadata": {
        "id": "JuW-eOGu2tPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CPU로"
      ],
      "metadata": {
        "id": "5_TFQX1B3GYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctAyHHDL1Yx8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train()\n",
        "\n",
        "# 2. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 3. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU로"
      ],
      "metadata": {
        "id": "ZWtGB9-z2AI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.utils.serialization as xser\n",
        "\n",
        "# 1. TPU 장치 설정\n",
        "device = xm.xla_device()\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 4. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "FKghI9dw3LzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU로"
      ],
      "metadata": {
        "id": "rwOhPEgw3NQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = (anchor - positive).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        neg_distance = (anchor - negative).pow(2).sum(1)  # 유클리드 거리의 제곱\n",
        "        loss = torch.relu(pos_distance - neg_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# 4. 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = os.listdir(anchor_dir)\n",
        "        self.positive_images = os.listdir(positive_dir)\n",
        "        self.negative_images = os.listdir(negative_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.anchor_images), len(self.positive_images), len(self.negative_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = os.path.join(self.anchor_dir, self.anchor_images[idx])\n",
        "        positive_path = os.path.join(self.positive_dir, self.positive_images[idx])\n",
        "        negative_path = os.path.join(self.negative_dir, self.negative_images[idx])\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "UVZ3jEUn3O8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링"
      ],
      "metadata": {
        "id": "2mUvurFr5Lv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 검색어 설정\n",
        "query = '한국 연예인'  # 검색어를 변경할 수 있습니다.\n",
        "url = f'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={query}'\n",
        "\n",
        "# 요청 헤더 설정 (네이버는 User-Agent 확인을 통해 봇을 차단할 수 있음)\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 이미지 URL 수집\n",
        "image_urls = []\n",
        "for img_tag in soup.find_all('img'):\n",
        "    try:\n",
        "        img_url = img_tag['data-source']\n",
        "        image_urls.append(img_url)\n",
        "    except KeyError:\n",
        "        # data-source 속성이 없는 경우 건너뜀\n",
        "        continue\n",
        "\n",
        "# 이미지 다운로드 디렉터리 설정\n",
        "os.makedirs('korean_celeb_images', exist_ok=True)\n",
        "\n",
        "# 이미지 다운로드\n",
        "for i, img_url in enumerate(image_urls):\n",
        "    try:\n",
        "        img_response = requests.get(img_url)\n",
        "        img_response.raise_for_status()  # 요청이 성공했는지 확인\n",
        "        with open(f'korean_celeb_images/korean_celeb_{i}.jpg', 'wb') as file:\n",
        "            file.write(img_response.content)\n",
        "        print(f'Downloaded image {i + 1}')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f'Failed to download image {i + 1}: {e}')\n",
        "\n",
        "print('이미지 다운로드 완료')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDSQ2uPg5NP8",
        "outputId": "66fdaf8d-eef1-4428-b9ba-52c424cfd46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이미지 다운로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPsf3xqLmNQ",
        "outputId": "4a9e03d2-44a7-44dd-dab2-62d058bcc8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.31.0)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.2.2)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aOGJ8EoE6CN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "outputId": "7744b9fc-0b04-4701-f932-a464c314683d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이미지 찾을 대상을 입력해주세요.백종원\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-567b99b96305>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 크롬 드라이버 설치\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 구글 사이트 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/chrome.py\u001b[0m in \u001b[0;36minstall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdriver_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_driver_binary_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o755\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdriver_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/core/manager.py\u001b[0m in \u001b[0;36m_get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mos_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_os_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_driver_download_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mbinary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_file_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/drivers/chrome.py\u001b[0m in \u001b[0;36mget_driver_download_url\u001b[0;34m(self, os_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_driver_download_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdriver_version_to_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_driver_version_to_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# For Mac ARM CPUs after version 106.0.5249.61 the format of OS type changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# to more unified \"mac_arm64\". For newer versions, it'll be \"mac_arm64\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/core/driver.py\u001b[0m in \u001b[0;36mget_driver_version_to_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver_version_to_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latest_release_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_latest_release_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webdriver_manager/drivers/chrome.py\u001b[0m in \u001b[0;36mget_latest_release_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdetermined_browser_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Remove the build version (the last segment) from determined_browser_version for version < 113\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdetermined_browser_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetermined_browser_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         latest_release_url = (\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_release_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, positive_dir, negative_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.positive_dir = positive_dir\n",
        "        self.negative_dir = negative_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = sorted(os.listdir(anchor_dir))\n",
        "        self.positive_images = sorted(os.listdir(positive_dir))\n",
        "        self.negative_images = sorted(os.listdir(negative_dir))\n",
        "\n",
        "        # 앵커 이미지에 해당하는 양성 이미지를 매핑합니다.\n",
        "        self.anchor_to_positives = self._map_anchor_to_positives()\n",
        "\n",
        "    def _map_anchor_to_positives(self):\n",
        "        anchor_to_positives = {}\n",
        "        for positive_image in self.positive_images:\n",
        "            anchor_name = positive_image.split('_')[0]\n",
        "            if anchor_name not in anchor_to_positives:\n",
        "                anchor_to_positives[anchor_name] = []\n",
        "            anchor_to_positives[anchor_name].append(positive_image)\n",
        "        return anchor_to_positives\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_image_name = self.anchor_images[idx]\n",
        "        anchor_path = os.path.join(self.anchor_dir, anchor_image_name)\n",
        "\n",
        "        # 앵커 이미지에 대응하는 양성 이미지 중 하나를 랜덤하게 선택합니다.\n",
        "        positive_image_name = random.choice(self.anchor_to_positives[anchor_image_name.split('.')[0]])\n",
        "        positive_path = os.path.join(self.positive_dir, positive_image_name)\n",
        "\n",
        "        # 음성 이미지는 무작위로 선택합니다.\n",
        "        negative_image_name = random.choice(self.negative_images)\n",
        "        negative_path = os.path.join(self.negative_dir, negative_image_name)\n",
        "\n",
        "        anchor_image = Image.open(anchor_path).convert('RGB')\n",
        "        positive_image = Image.open(positive_path).convert('RGB')\n",
        "        negative_image = Image.open(negative_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)\n",
        "            positive_image = self.transform(positive_image)\n",
        "            negative_image = self.transform(negative_image)\n",
        "\n",
        "        return anchor_image, positive_image, negative_image\n",
        "\n",
        "# 이미지 전처리 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 폴더 경로 설정 (실제 경로로 대체 필요)\n",
        "anchor_dir = 'path/to/dataset/anchor'\n",
        "positive_dir = 'path/to/dataset/positive'\n",
        "negative_dir = 'path/to/dataset/negative'\n",
        "\n",
        "# 데이터 로더 정의\n",
        "dataset = TripletDataset(anchor_dir, positive_dir, negative_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "v-GfANlELhHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트립렛 손실 함수 정의\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(dataset_path)\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        imgs = []\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.dataset_path, class_name)\n",
        "            img_names = os.listdir(class_path)\n",
        "            anchor_imgs = [img_name for img_name in img_names if 'anchor' in img_name]\n",
        "            for anchor_img in anchor_imgs:\n",
        "                positive_imgs = [img_name for img_name in img_names if img_name != anchor_img and 'positive' in img_name]\n",
        "                negative_imgs = [img_name for img_name in img_names if 'negative' in img_name]\n",
        "                for positive_img in positive_imgs:\n",
        "                    for negative_img in negative_imgs:\n",
        "                        imgs.append((os.path.join(class_path, anchor_img), os.path.join(class_path, positive_img), os.path.join(class_path, negative_img)))\n",
        "        return imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_path, positive_path, negative_path = self.imgs[index]\n",
        "        anchor_img = Image.open(anchor_path)\n",
        "        positive_img = Image.open(positive_path)\n",
        "        negative_img = Image.open(negative_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        embedding = model(image.unsqueeze(0))\n",
        "    return embedding\n",
        "\n",
        "# 예시: 앵커, 양성, 음성 이미지의 임베딩 벡터 계산\n",
        "anchor_image = transform(Image.open('path/to/anchor_image.jpg').convert('RGB'))\n",
        "positive_image = transform(Image.open('path/to/positive_image.jpg').convert('RGB'))\n",
        "negative_image = transform(Image.open('path/to/negative_image.jpg').convert('RGB'))\n",
        "\n",
        "anchor_embedding = get_embedding(model, anchor_image)\n",
        "positive_embedding = get_embedding(model, positive_image)\n",
        "negative_embedding = get_embedding(model, negative_image)\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "pos_distance = torch.dist(anchor_embedding, positive_embedding, p=2).item()\n",
        "neg_distance = torch.dist(anchor_embedding, negative_embedding, p=2).item()\n",
        "\n",
        "# 임계값을 기준으로 분류\n",
        "threshold = 0.5  # 임계값 설정\n",
        "if pos_distance < threshold:\n",
        "    print(\"Positive (같은 클래스)\")\n",
        "else:\n",
        "    print(\"Negative (다른 클래스)\")\n",
        "\n",
        "print(f'Positive Distance: {pos_distance}')\n",
        "print(f'Negative Distance: {neg_distance}')\n"
      ],
      "metadata": {
        "id": "z6JImhd--mdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1. GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 모델 정의\n",
        "model = InceptionResnetV1(pretrained='vggface2').train().to(device)\n",
        "\n",
        "# 3. 트리플렛 손실 함수와 최적화 함수 정의\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    # 기존의 코드와 동일합니다.\n",
        "    ...\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = TripletDataset('dataset/', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        anchor, positive, negative = data\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding = model(anchor)\n",
        "        positive_embedding = model(positive)\n",
        "        negative_embedding = model(negative)\n",
        "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 9:  # 매 10 미니 배치마다 출력\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 4. 임베딩 벡터 계산 및 분류 함수 정의\n",
        "def get_embedding(model, image):\n",
        "    # 기존의 코드와 동일합니다.\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "dhaMDNaAAIQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네, 제공하신 구조에 따라 이미지를 저장하는 방법은 트리플렛 네트워크 학습에 적합합니다. 각 '앵커' 폴더에는 해당 앵커 이미지와 그에 대응하는 긍정(positive) 및 부정(negative) 이미지가 포함되어 있습니다. 이 구조는 앵커 이미지와 긍정 이미지 간의 유사성을 학습하고, 앵커 이미지와 부정 이미지 간의 차이를 학습하는 데 도움이 됩니다.\n",
        "\n",
        "# 그러나, 실제 코드에서 이미지 경로를 만들 때 'anchor', 'positive', 'negative'라는 단어를 포함하는 방식에 따라 이미지를 분류하므로, 폴더 구조와 이미지의 이름을 명확하게 정의해야 합니다. 제공하신 예시에서는 각 앵커 폴더에 하나의 'anchor.jpg' 이미지, 여러 개의 'positive_xx.jpg', 그리고 여러 개의 'negative_xx.jpg' 이미지가 포함되어 있습니다. 이 구조는 코드에서 정의한 `TripletDataset` 클래스의 `make_dataset` 메소드 로직과 일치합니다.\n",
        "\n",
        "# `TripletDataset` 클래스의 `make_dataset` 메소드는 각 앵커 이미지에 대해 가능한 모든 긍정 및 부정 이미지 조합을 찾아 이미지의 트리플렛(앵커, 긍정, 부정)을 생성합니다. 이렇게 하여 학습 데이터셋에 다양성을 부여하고, 모델이 더 강력한 특징을 학습할 수 있도록 합니다.\n",
        "\n",
        "# 따라서, 제공하신 파일 구조는 코드의 요구사항을 충족하며, 트리플렛 네트워크 학습에 적합한 방식입니다. 하지만, 실제 프로젝트에서는 클래스별로 더 많은 이미지를 포함시키고, 다양한 앵커, 긍정, 부정 이미지 조합을 실험해 볼 필요가 있습니다. 이를 통해 모델의 일반화 능력을 향상시키고, 실제 세계의 다양한 시나리오에 대응할 수 있는 robust한 모델을 개발할 수 있습니다.\n",
        "\n",
        "# 이런 자료를 참고했어요.\n",
        "# [1] AI-Hub - 한국어 이미지 설명 데이터셋 (https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=261)\n",
        "# [2] 티스토리 - [PyTorch] 4-1. 나만의 이미지 데이터셋 만들기 - Real Late Starter (https://data-panic.tistory.com/13)\n",
        "# [3] Superb AI - 컴퓨터 비전 데이터셋 - 공공 데이터셋 살펴보기 - 슈퍼브 블로그 (https://blog-ko.superb-ai.com/exploring-computer-vision-datasets/)\n",
        "# [4] GitHub - 작은 데이터셋으로 강력한 이미지 분류 모델 설계하기 (https://keraskorea.github.io/posts/2018-10-24-little_data_powerful_model/)\n",
        "\n",
        "# 뤼튼 사용하러 가기 > https://agent.wrtn.ai/5xb91l"
      ],
      "metadata": {
        "id": "UsvQ3njvAOmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}